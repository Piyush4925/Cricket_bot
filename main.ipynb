{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UHtZ3cE0UBz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from bitsandbytes.nn import Linear4bit\n",
        "from pypdf2 import PdfReader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "girw50S30Wnd",
        "outputId": "29b61986-79e9-4ce0-a493-e706d366aabd"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.4' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python312/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Read and process PDFs\n",
        "def read_pdfs(file_paths):\n",
        "    text = \"\"\n",
        "    for file_path in file_paths:\n",
        "        with open(file_path, 'rb') as file:\n",
        "            reader = PdfReader(file)\n",
        "            for page in reader.pages:\n",
        "                text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "\n",
        "# Load multiple PDFs\n",
        "pdf_files = [\"path/to/pdf1.pdf\", \"path/to/pdf2.pdf\"]  # Add more file paths as needed\n",
        "pdf_text = read_pdfs(pdf_files)\n",
        "\n",
        "# Split into smaller chunks for processing (optional)\n",
        "text_chunks = [pdf_text[i:i+500] for i in range(0, len(pdf_text), 500)]\n",
        "\n",
        "# Store text chunks (you can use a list or database)\n",
        "\n",
        "stored_data = text_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ej3lvLbP0hyr",
        "outputId": "cef170bf-e00c-4999-b7b5-609f1951482b"
      },
      "outputs": [],
      "source": [
        "# Load quantized LLaMA model and tokenizer\n",
        "model_id = \"TheBloke/Llama-2-7B-Chat-GGUF\"  # Use the correct model ID from Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True, device_map='auto')\n",
        "\n",
        "# Replace Linear layers with 4-bit quantized versions\n",
        "for module in model.modules():\n",
        "    if isinstance(module, torch.nn.Linear):\n",
        "        module.__class__ = Linear4bit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1tmnADH60dN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
